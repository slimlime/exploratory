## 4/7/2017 Optimization

Rather than fix the current set of bugs or work on new features I have decided to work on memory optimization. To this end I will build a function which tabulates the memory usage by namespace in an effort to find the best bits to concentrate on.

Here are the results of the new dump-namespaces function

~~~~

DUNGEON-CELL (2275)
TYPESET-CS (743)
PARSER (445)
DECOMPRESS (206)
DISPATCHER (189)
TYPESET (123)
MEMSET (116)
MEMCPY (100)
BINARY-PARSER (43)
$0D90 (41)
PRINT-MESSAGE (37)
$0D04 (36)
$0C5E (36)
$0C39 (36)
$0D9D (28)
$0D73 (28)
$0D3E (28)
$0D5B (23)
$0D09 (23)
$0CEC (23)
$0C63 (23)
$0C3E (23)

~~~~

Unsurprisingly, the game data for the dungeon cell takes up the most memory. This does not include the compressed string data, but since this is already compressed as best as I can, it doesn't matter. So here are the things to do.

- Put the state in the zero-page




## 4/7/2017 Local variable assignment

Yesterday I discovered an interesting online paper by David A Wheeler (https://www.dwheeler.com/6502/a-lang.txt) which described some strategies for a higher level language for programming the 6502. Although my project is about *ad-hoc* program construction and specifically *not* language development or compilation, there are a lot of points of cross-over. He even uses the term super-assembler or something similar, which is a term I have been using to describe the LISP code here.

One thing that stood out was his observation that careful use of the zero-page could enable efficient sharing of data between function calls. He describes a topological sort which ensures that 'local' variables occupy the same slot in the zero-page as the a function which takes them as input, this avoiding a copy.

This is something I have to do manually, with the aid of aliasing and assertions,

~~~~

(with-namespace :print-message
    (label :print)
    (alias :str '(:typeset-cs . :str)) 
    (alias :tmp :A0)
    (ensure-aliases-different :str :tmp '(:typeset . :raster))
  
~~~~

Here we can see this in action- the typeset-cs str variable is used in the print message routine, and also provides the input to the typeset routine. An assertion follows to ensure that non of the zero-page aliases clash, both in the print message function and the function it calls.

Because we are rarely calling functions more than one level deep, it is relatively straightforward to ensure we do not have memory location clash manually. Should the call graph become more complex then the scheme described by Wheeler would be advantageous. At this point however, we would need to have a more concrete notion of what constitutes a function. Yagni, for now, and my guess is that the adventure game project will never have a call graph of functions which rises to this level of complexity.

## 3/7/2017 Esoteric Considerations

What is programming?

If one imagines a cross-section of the different programs in existence one will find that they all share some things in common. Namely, state and changes to state. Put alternatively, state and behaviour. Fundamentally I think this is what computation is all about. Database app, web-sites, games all take, manipulate and deliver state.

But what about pure functional languages? Surely they aren't about manipulating state? Absolutely they are. They do not exist in a vacuum- their state is the input provided to them and the output they give. Conceptually the sausage factory in the middle can be described as functions which have no side-effects but IMO this is a distinction without a difference. Functional concepts are absolutely essential in arranging ones thoughts about computation, but they are not fundamental to its implementation. Definite advantages can be had by thinking about a computational process in terms of pure function composition, but it is a category error to insist this be the basis for a *real* programming language.

Early programs (some of mine from the 80s for example) are a mess of state and gotos; they are basically impossible to fully understand. Programs such as this have a combinatorial explosion of state such that it is easy to get into the soup and have no idea how it happened. The only solution is to turn it off and on again. Absolutely no-one who has experienced this wants to go back to it.

I have learned programming this 6502 adventure game so far is this,

- Most bugs were in behaviour code- i.e. either incorrect design or just flat out incorrectly implemented logic.
- TWO 'state' bugs that went undetected. One was an uninitialised loop counter, the other was the parse input buffer not being cleared down. This is not indicative of a combinatorial explosion of dodgy state.
- Unit testing is good to ensure behaviour works
- Unit testing still can't deal with unknown state transitions

So,

- Functional thinking is useful *in the conceptual or design realm*. Make a serious effort to reduce the amount of state, and therefore the number of state transitions.
- Functional straitjackets *in the implementation realm* are not useful. Loop, modify state, do what needs to be done, get over it.

### Fetishization of recursion.

Recursion is over-rated. There are *very* few problems which require recursion, for example, there is no recursion in the code-base for this adventure game. None. Scheme, Haskell (and most modern LISP) tutorials would have us believe that if we aren't doing recursion to add a list of numbers together then we are like the apes at the beginning of the 2001 movie.

Do we need recursion?

Can you type a recursive solution faster than you can type a loop based solution? If so, do it. Did we *need* to do it recursively? No. Is it faster? Probably not, most languages don't have tail-call optimisation.

Ok, my language has tail-call optimisation. Great! You might even get it as a benefit of any JITting that occurs. Now, did you remember to put the recursive call in the tail position? Not sure? Is it even possible? If it's not possible, you may be able to do it as a loop- if not congratulations, you may need recursion!
 
Ok, we need recursion, we can't use TCO as this is a problem that requires some state to be saved until after the recursive call. Now, there is but one question. Do you have enough stack to solve the problem? No, make it two questions. Can you arrange the code so that it doesn't melt your brain? This is not an idle consideration. In my programming career I have written enough recursive code that really did need to store up the state, and the primary worry was whether it would scale. The secondary worry was arranging the functions just so, so that the shape of the call stack fit the shape of the problem.

If the problem doesn't fit the shape of the call stack well, or if the call stack would not be large enough you may need to 'emulate' recursion by using some sort of queue, or explicit stack and... a loop. At this point I guarantee your code will not have the idiomatic shape of recursive code in your chosen language.

Now my code isn't functional! How can I compose it and take advantage of all the gifts and boons this bestows? Just concentrate on make your function pure, if composing it and keeping it nice and modular is your concern. Don't worry about the seething mass of state inside, no-one else will ever see it.

Now my code isn't functional! How can I compose it with other behaviour INSIDE the recursive code. Have you ever tried to do this as a solution to anything outside a toy problem? I think there are probably five people in the world who can do this with ease and they are either working on the Haskell compiler or their PhD thesis is literally called "Mutually Recursive Functions for Fun and Recreation". Of course, for this thesis, they have written their own Scheme implementation in Python, and the recursive bits are transpiled to imperative code, just for now, until they can make it more performant.

Let us examine three Fibonaccis in SBCL LISP.

- Loop based
- Recursive

Conclusion

Recursion is (mostly) only useful for toy problems of the sort found in modern programming tutorials, but may find occasional use where the problem is well suited to being expressed recursively and has no scaling problems. Mutual recursion in anything more complex than working out Fibonacci is basically far too complex to arrange in practice. Sadly I think that LISP tutorials feature too much emphasis on mapcar and pals- a construct which is not very efficient. It makes more sense to instruct functional programming in C# using LINQ, but even there we see the limits of composability very quickly. Going beyond a few chained Selects and Wheres is a painful nightmare in practice. Grouping? Forget about it.

Now let us look at a recursive solution with memoization, which will lead us to the other thing that programming is.

- Recursion with memoization

## Programming is transformation of state

## Transformation of state is Data de-compression

## Data de-compression is prediction

## Prediction is action

And now we have arrived at either a profoundly strange loop, or a humdrum tautology.

