## 4/7/2017 Optimization

Rather than fix the current set of bugs or work on new features I have decided to work on memory optimization. To this end I will build a function which tabulates the memory usage by namespace in an effort to find the best bits to concentrate on.

Here are the results of the new dump-namespaces function

~~~~

DUNGEON-CELL (2275)
TYPESET-CS (743)
PARSER (445)
DECOMPRESS (206)
DISPATCHER (189)
...

~~~~

Unsurprisingly, the game data for the dungeon cell takes up the most memory. This does not include the compressed string data, but since this is already compressed as best as I can, it doesn't matter. So here are the things to do.

- Put the state in the zero-page (DONE)

~~~~

DUNGEON-CELL (2228)

~~~~

- Alternate mechanism for state

Current method for setting state sets the entire byte to either 0 or FF.

~~~~

4 bytes to set, 4 bytes to clear.

LDA #xFF
STA $byte

LDA #x00
STA $byte

~~~~

This requires loading the accumulator with the right value. Alternatively, we can use the property that BIT also tests the sign bit of the byte, so the following trick becomes available.

~~~~

3 bytes set, 2 bytes clear! Rotate in the carry to set the sign bit, shift the byte to clear it.

SEC
ROR $byte

LSR $byte

~~~~

Disadvantage of this method is that it only ensures that the sign bit is set or clear. This means that there are many more instructions that will affect it. This means we will probably have to do a three byte jump to skip the else clause where required.

And now we are down to...

~~~~

DUNGEON-CELL (2195)

~~~~

- Branch directly to RTS (TODO)

The if macro puts a branch in to skip past the else clause; sometimes this goes directly to some more code, which is fine, but other times it branches directly to an RTS. Sometimes this is more than one level deep, a waste of 2 bytes each time this occurs.

e.g. the JMPs at 0C44 and 0C4C are very dull, and could be replaced with an RTS.

~~~~

N-CELL:EXAMINE-CRACK 0C34 2412    BIT $12    ;IF SLIME-LICKED
         $0C1F:ENDIF 0C36 1017    BPL $0C4F  ;$0C34:ELSE 
                     0C38 2415    BIT $15    ;IF KEY-IN-CRACK
                     0C3A 100B    BPL $0C47  ;$0C38:ELSE 
                     0C3C 20160E  JSR $0E16  ;PRINT-MESSAGE:1 
                     0C3F 0B..    DW $180B   ;A glint of metal shines back at you... A key!
                     0C41 38      SEC
                     0C42 6611    ROR $11
                     0C44 4C4C0C  JMP $0C4C  ;$0C38:ENDIF 
          $0C38:ELSE 0C47 20C30D  JSR $0DC3  ;PRINT-MESSAGE:3 
                     0C4A C7..    DW $17C7   ;A crack in the floor, just like any other.
One might hide a small key-like object here.
Like, for example, a key.
         $0C38:ENDIF 0C4C 4C540C  JMP $0C54  ;$0C34:ENDIF $0C37:ELSE 
          $0C34:ELSE 0C4F 20C30D  JSR $0DC3  ;PRINT-MESSAGE:3 
                     0C52 97..    DW $1797   ;The Veil of Maia, or your shocking
hangover prevents you from seeing
anything interesting.
          $0C37:ELSE
         $0C34:ENDIF 0C54 60      RTS

~~~~

If we were doing compilation there would be an intermediate step which would analyse the branches and collapse redundant ones. Since we are only doing assembly, there may be a trick we can do, but I am going to put it on the back burner.

- Parameterize location initialization (TODO)

The pre-amble to setting up a location is long and tedious (approx 150 bytes). Considering the location has only an image, a title and a dispatch table, it really should just be 6 bytes plus a JSR.

We can make it into a function, but in the normal way I will wait until there is at least another instance.

## 4/7/2017 Local variable assignment

Yesterday I discovered an interesting online paper by David A Wheeler (https://www.dwheeler.com/6502/a-lang.txt) which described some strategies for a higher level language for programming the 6502. Although my project is about *ad-hoc* program construction and specifically *not* language development or compilation, there are a lot of points of cross-over. He even uses the term super-assembler or something similar, which is a term I have been using to describe the LISP code here.

One thing that stood out was his observation that careful use of the zero-page could enable efficient sharing of data between function calls. He describes a topological sort which ensures that 'local' variables occupy the same slot in the zero-page as the a function which takes them as input, this avoiding a copy.

This is something I have to do manually, with the aid of aliasing and assertions,

~~~~

(with-namespace :print-message
    (label :print)
    (alias :str '(:typeset-cs . :str)) 
    (alias :tmp :A0)
    (ensure-aliases-different :str :tmp '(:typeset . :raster))
  
~~~~

Here we can see this in action- the typeset-cs str variable is used in the print message routine, and also provides the input to the typeset routine. An assertion follows to ensure that non of the zero-page aliases clash, both in the print message function and the function it calls.

Because we are rarely calling functions more than one level deep, it is relatively straightforward to ensure we do not have memory location clash manually. Should the call graph become more complex then the scheme described by Wheeler would be advantageous. At this point however, we would need to have a more concrete notion of what constitutes a function. Yagni, for now, and my guess is that the adventure game project will never have a call graph of functions which rises to this level of complexity.

## 3/7/2017 Esoteric Considerations

What is programming?

If one imagines a cross-section of the different programs in existence one will find that they all share some things in common. Namely, state and changes to state. Put alternatively, state and behaviour. Fundamentally I think this is what computation is all about. Database app, web-sites, games all take, manipulate and deliver state.

But what about pure functional languages? Surely they aren't about manipulating state? Absolutely they are. They do not exist in a vacuum- their state is the input provided to them and the output they give. Conceptually the sausage factory in the middle can be described as functions which have no side-effects but IMO this is a distinction without a difference. Functional concepts are absolutely essential in arranging ones thoughts about computation, but they are not fundamental to its implementation. Definite advantages can be had by thinking about a computational process in terms of pure function composition, but it is a category error to insist this be the basis for a *real* programming language.

Early programs (some of mine from the 80s for example) are a mess of state and gotos; they are basically impossible to fully understand. Programs such as this have a combinatorial explosion of state such that it is easy to get into the soup and have no idea how it happened. The only solution is to turn it off and on again. Absolutely no-one who has experienced this wants to go back to it.

I have learned programming this 6502 adventure game so far is this,

- Most bugs were in behaviour code- i.e. either incorrect design or just flat out incorrectly implemented logic.
- TWO 'state' bugs that went undetected. One was an uninitialised loop counter, the other was the parse input buffer not being cleared down. This is not indicative of a combinatorial explosion of dodgy state.
- Unit testing is good to ensure behaviour works
- Unit testing still can't deal with unknown state transitions

So,

- Functional thinking is useful *in the conceptual or design realm*. Make a serious effort to reduce the amount of state, and therefore the number of state transitions.
- Functional straitjackets *in the implementation realm* are not useful. Loop, modify state, do what needs to be done, get over it.

### Fetishization of recursion.

Recursion is over-rated. There are *very* few problems which require recursion, for example, there is no recursion in the code-base for this adventure game. None. Scheme, Haskell (and most modern LISP) tutorials would have us believe that if we aren't doing recursion to add a list of numbers together then we are like the apes at the beginning of the 2001 movie.

Do we need recursion?

Can you type a recursive solution faster than you can type a loop based solution? If so, do it. Did we *need* to do it recursively? No. Is it faster? Probably not, most languages don't have tail-call optimisation.

Ok, my language has tail-call optimisation. Great! You might even get it as a benefit of any JITting that occurs. Now, did you remember to put the recursive call in the tail position? Not sure? Is it even possible? If it's not possible, you may be able to do it as a loop- if not congratulations, you may need recursion!
 
Ok, we need recursion, we can't use TCO as this is a problem that requires some state to be saved until after the recursive call. Now, there is but one question. Do you have enough stack to solve the problem? No, make it two questions. Can you arrange the code so that it doesn't melt your brain? This is not an idle consideration. In my programming career I have written enough recursive code that really did need to store up the state, and the primary worry was whether it would scale. The secondary worry was arranging the functions just so, so that the shape of the call stack fit the shape of the problem.

If the problem doesn't fit the shape of the call stack well, or if the call stack would not be large enough you may need to 'emulate' recursion by using some sort of queue, or explicit stack and... a loop. At this point I guarantee your code will not have the idiomatic shape of recursive code in your chosen language.

Now my code isn't functional! How can I compose it and take advantage of all the gifts and boons this bestows? Just concentrate on make your function pure, if composing it and keeping it nice and modular is your concern. Don't worry about the seething mass of state inside, no-one else will ever see it.

Now my code isn't functional! How can I compose it with other behaviour INSIDE the recursive code. Have you ever tried to do this as a solution to anything outside a toy problem? I think there are probably five people in the world who can do this with ease and they are either working on the Haskell compiler or their PhD thesis is literally called "Mutually Recursive Functions for Fun and Recreation". Of course, for this thesis, they have written their own Scheme implementation in Python, and the recursive bits are transpiled to imperative code, just for now, until they can make it more performant.

Let us examine three Fibonaccis in SBCL LISP.

- Loop based
- Recursive

Conclusion

Recursion is (mostly) only useful for toy problems of the sort found in modern programming tutorials, but may find occasional use where the problem is well suited to being expressed recursively and has no scaling problems. Mutual recursion in anything more complex than working out Fibonacci is basically far too complex to arrange in practice. Sadly I think that LISP tutorials feature too much emphasis on mapcar and pals- a construct which is not very efficient. It makes more sense to instruct functional programming in C# using LINQ, but even there we see the limits of composability very quickly. Going beyond a few chained Selects and Wheres is a painful nightmare in practice. Grouping? Forget about it.

Now let us look at a recursive solution with memoization, which will lead us to the other thing that programming is.

- Recursion with memoization

## Programming is transformation of state

## Transformation of state is Data de-compression

## Data de-compression is prediction

## Prediction is closely related to action, which is transformation of state

And now we have arrived at either a profoundly strange loop, or a humdrum tautology.

